
Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/5.0.5

The following have been reloaded with a version change:
  1) gcc/14.1.0 => gcc/11.4.1

+ NUM_EPISODES=3
+ n_samples_per_prompt=8
+ n_rollout_max=8
+ n_rollout_min=8
+ LR_ACTOR=5e-6
+ entropy_coeff=0
+ n_rollout_update=0
+ enable_temperature_scheduler=False
+ enable_annealing=False
+ TRAIN_DATADIR=./dataset/train_data_10k.parquet
+ VAL_DATADIR=./dataset/valid_data.parquet
+ MODELDIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ PRETRAIN_DIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ SAVE_DIR=../checkpoint/e3_1_5b_grpo/
+ TENSORBOARD_PATH=../checkpoint/e3_1_5b_grpo//tensorboard
+ export TENSORBOARD_DIR=../checkpoint/e3_1_5b_grpo//tensorboard
+ TENSORBOARD_DIR=../checkpoint/e3_1_5b_grpo//tensorboard
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ python3 -m e3.main_e3 algorithm.adv_estimator=grpo data.train_files=./dataset/train_data_10k.parquet data.val_files=./dataset/valid_data.parquet data.train_batch_size=64 data.max_prompt_length=2048 data.max_response_length=6144 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B actor_rollout_ref.actor.optim.lr=5e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=512 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.8 actor_rollout_ref.rollout.n=8 actor_rollout_ref.rollout.n_low=8 actor_rollout_ref.rollout.n_high=8 actor_rollout_ref.rollout.n_update=0 actor_rollout_ref.rollout.temperature=1 actor_rollout_ref.rollout.enable_temperature_scheduler=False actor_rollout_ref.rollout.enable_annealing=False actor_rollout_ref.rollout.max_steps=480 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 'trainer.logger=[console,tensorboard]' trainer.project_name=GRPO trainer.experiment_name=Qwen trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.default_local_dir=../checkpoint/e3_1_5b_grpo/ trainer.save_freq=50 trainer.test_freq=10 trainer.total_epochs=3
2025-11-06 21:16:00,340	INFO worker.py:2003 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(TaskRunner pid=555874)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 21:16:31,517:Waiting for register center actor tQYokd_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=565542)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(WorkerDict pid=565198)[0m   warnings.warn(  # warn only once
[36m(WorkerDict pid=565198)[0m [rank0]:[W1106 21:16:53.297765554 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[36m(WorkerDict pid=565543)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=565543)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=565198)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m   warnings.warn(  # warn only once[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m [rank3]:[W1106 21:16:53.566511408 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565198)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(WorkerDict pid=565198)[0m   warnings.warn(  # warn only once
[36m(WorkerDict pid=565542)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(WorkerDict pid=565542)[0m   warnings.warn(  # warn only once
[36m(WorkerDict pid=565542)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=565544)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=565544)[0m   warnings.warn(  # warn only once[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=565198)[0m   warnings.warn(
[36m(WorkerDict pid=565198)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=555874)[0m Training Progress:   0%|          | 0/480 [00:00<?, ?it/s]
[36m(WorkerDict pid=565542)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565542)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=555874)[0m Training Progress:   0%|          | 1/480 [01:57<15:38:17, 117.53s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   0%|          | 2/480 [03:57<15:47:25, 118.92s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   1%|          | 3/480 [05:59<15:58:17, 120.54s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   1%|          | 4/480 [07:56<15:45:15, 119.15s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   1%|          | 5/480 [09:54<15:39:59, 118.74s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   1%|â–         | 6/480 [12:00<15:55:43, 120.98s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   1%|â–         | 7/480 [14:06<16:08:12, 122.82s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   2%|â–         | 8/480 [16:07<15:59:51, 122.02s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   2%|â–         | 9/480 [18:04<15:46:39, 120.59s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   2%|â–         | 10/480 [21:23<18:55:00, 144.89s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   2%|â–         | 11/480 [23:19<17:43:37, 136.07s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   2%|â–Ž         | 12/480 [25:17<16:58:02, 130.52s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   3%|â–Ž         | 13/480 [27:15<16:26:28, 126.74s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   3%|â–Ž         | 14/480 [29:19<16:16:22, 125.71s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   3%|â–Ž         | 15/480 [31:15<15:53:27, 123.03s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   3%|â–Ž         | 16/480 [33:15<15:42:46, 121.91s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   4%|â–Ž         | 17/480 [35:14<15:34:21, 121.08s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   4%|â–         | 18/480 [37:13<15:28:03, 120.53s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   4%|â–         | 19/480 [39:13<15:24:02, 120.27s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   4%|â–         | 20/480 [42:34<18:29:18, 144.69s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   4%|â–         | 21/480 [44:34<17:30:05, 137.27s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   5%|â–         | 22/480 [46:34<16:48:11, 132.08s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   5%|â–         | 23/480 [48:32<16:13:37, 127.83s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   5%|â–Œ         | 24/480 [50:28<15:44:04, 124.22s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   5%|â–Œ         | 25/480 [52:23<15:21:22, 121.50s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   5%|â–Œ         | 26/480 [54:20<15:07:47, 119.97s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   6%|â–Œ         | 27/480 [56:21<15:08:31, 120.34s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   6%|â–Œ         | 28/480 [58:13<14:47:12, 117.77s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   6%|â–Œ         | 29/480 [1:00:12<14:47:59, 118.14s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   6%|â–‹         | 30/480 [1:03:21<17:25:35, 139.41s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 22:23:51,716:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:   6%|â–‹         | 31/480 [1:05:16<16:29:49, 132.27s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   7%|â–‹         | 32/480 [1:07:03<15:31:00, 124.69s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   7%|â–‹         | 33/480 [1:08:59<15:09:14, 122.05s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   7%|â–‹         | 34/480 [1:10:54<14:50:29, 119.80s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   7%|â–‹         | 35/480 [1:12:46<14:32:48, 117.68s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   8%|â–Š         | 36/480 [1:14:36<14:12:28, 115.20s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   8%|â–Š         | 37/480 [1:16:29<14:06:38, 114.67s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   8%|â–Š         | 38/480 [1:18:28<14:13:16, 115.83s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   8%|â–Š         | 39/480 [1:20:16<13:53:40, 113.42s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 22:40:47,378:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:   8%|â–Š         | 40/480 [1:23:30<16:49:03, 137.60s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   9%|â–Š         | 41/480 [1:25:19<15:43:56, 129.01s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   9%|â–‰         | 42/480 [1:27:12<15:08:12, 124.41s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   9%|â–‰         | 43/480 [1:29:00<14:29:40, 119.41s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   9%|â–‰         | 44/480 [1:30:55<14:17:02, 117.94s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:   9%|â–‰         | 45/480 [1:32:40<13:47:52, 114.19s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  10%|â–‰         | 46/480 [1:34:31<13:39:18, 113.27s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  10%|â–‰         | 47/480 [1:36:20<13:27:35, 111.91s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  10%|â–ˆ         | 48/480 [1:38:11<13:25:00, 111.81s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  10%|â–ˆ         | 49/480 [1:40:06<13:28:22, 112.53s/it]
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=565198)[0m   warnings.warn(
[36m(TaskRunner pid=555874)[0m Training Progress:  10%|â–ˆ         | 50/480 [1:43:27<16:36:32, 139.05s/it]
[36m(WorkerDict pid=565544)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=555874)[0m Training Progress:  11%|â–ˆ         | 51/480 [1:45:14<15:27:11, 129.68s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  11%|â–ˆ         | 52/480 [1:47:03<14:39:03, 123.23s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  11%|â–ˆ         | 53/480 [1:48:46<13:54:16, 117.23s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  11%|â–ˆâ–        | 54/480 [1:50:40<13:45:47, 116.31s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  11%|â–ˆâ–        | 55/480 [1:52:26<13:22:34, 113.30s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  12%|â–ˆâ–        | 56/480 [1:54:14<13:09:37, 111.74s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  12%|â–ˆâ–        | 57/480 [1:56:04<13:04:18, 111.25s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  12%|â–ˆâ–        | 58/480 [1:57:50<12:50:05, 109.49s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  12%|â–ˆâ–        | 59/480 [1:59:37<12:43:47, 108.85s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  12%|â–ˆâ–Ž        | 60/480 [2:02:31<14:57:19, 128.19s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  13%|â–ˆâ–Ž        | 61/480 [2:04:21<14:17:04, 122.73s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  13%|â–ˆâ–Ž        | 62/480 [2:06:05<13:37:43, 117.38s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  13%|â–ˆâ–Ž        | 63/480 [2:07:49<13:07:56, 113.37s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 23:28:18,218:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  13%|â–ˆâ–Ž        | 64/480 [2:09:48<13:15:48, 114.78s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  14%|â–ˆâ–Ž        | 65/480 [2:11:32<12:51:35, 111.56s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  14%|â–ˆâ–        | 66/480 [2:13:20<12:44:19, 110.77s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  14%|â–ˆâ–        | 67/480 [2:15:04<12:28:11, 108.70s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  14%|â–ˆâ–        | 68/480 [2:16:51<12:21:26, 107.98s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  14%|â–ˆâ–        | 69/480 [2:18:36<12:14:57, 107.29s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  15%|â–ˆâ–        | 70/480 [2:21:35<14:40:28, 128.85s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  15%|â–ˆâ–        | 71/480 [2:23:23<13:54:39, 122.44s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  15%|â–ˆâ–Œ        | 72/480 [2:25:08<13:17:39, 117.30s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  15%|â–ˆâ–Œ        | 73/480 [2:26:53<12:50:56, 113.65s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 23:47:22,744:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  15%|â–ˆâ–Œ        | 74/480 [2:28:48<12:51:02, 113.95s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  16%|â–ˆâ–Œ        | 75/480 [2:30:32<12:29:10, 110.99s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-06 23:50:59,271:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  16%|â–ˆâ–Œ        | 76/480 [2:32:25<12:30:45, 111.50s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  16%|â–ˆâ–Œ        | 77/480 [2:34:13<12:21:18, 110.37s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  16%|â–ˆâ–‹        | 78/480 [2:36:01<12:16:15, 109.89s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  16%|â–ˆâ–‹        | 79/480 [2:37:51<12:13:09, 109.70s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  17%|â–ˆâ–‹        | 80/480 [2:40:46<14:23:11, 129.48s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  17%|â–ˆâ–‹        | 81/480 [2:42:34<13:36:56, 122.85s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  17%|â–ˆâ–‹        | 82/480 [2:44:19<12:59:59, 117.59s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  17%|â–ˆâ–‹        | 83/480 [2:46:06<12:37:27, 114.48s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  18%|â–ˆâ–Š        | 84/480 [2:47:52<12:18:33, 111.90s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  18%|â–ˆâ–Š        | 85/480 [2:49:38<12:05:49, 110.25s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  18%|â–ˆâ–Š        | 86/480 [2:51:25<11:57:41, 109.29s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  18%|â–ˆâ–Š        | 87/480 [2:53:13<11:52:46, 108.82s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  18%|â–ˆâ–Š        | 88/480 [2:55:04<11:54:57, 109.43s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  19%|â–ˆâ–Š        | 89/480 [2:56:49<11:44:48, 108.16s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  19%|â–ˆâ–‰        | 90/480 [2:59:47<13:58:49, 129.05s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  19%|â–ˆâ–‰        | 91/480 [3:01:28<13:02:28, 120.69s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 00:21:51,792:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  19%|â–ˆâ–‰        | 92/480 [3:03:16<12:34:33, 116.68s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  19%|â–ˆâ–‰        | 93/480 [3:05:03<12:13:47, 113.77s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 00:25:26,632:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  20%|â–ˆâ–‰        | 94/480 [3:06:55<12:09:23, 113.38s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  20%|â–ˆâ–‰        | 95/480 [3:08:38<11:48:29, 110.41s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  20%|â–ˆâ–ˆ        | 96/480 [3:10:26<11:41:15, 109.57s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  20%|â–ˆâ–ˆ        | 97/480 [3:12:07<11:23:33, 107.08s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  20%|â–ˆâ–ˆ        | 98/480 [3:13:44<11:01:02, 103.83s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  21%|â–ˆâ–ˆ        | 99/480 [3:15:33<11:09:47, 105.48s/it]
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=565198)[0m   warnings.warn(
[36m(WorkerDict pid=565198)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is bfloat16. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=565198)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is bfloat16. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(TaskRunner pid=555874)[0m Training Progress:  21%|â–ˆâ–ˆ        | 100/480 [3:18:45<13:52:27, 131.44s/it]
[36m(WorkerDict pid=565544)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=555874)[0m Training Progress:  21%|â–ˆâ–ˆ        | 101/480 [3:20:25<12:50:25, 121.97s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  21%|â–ˆâ–ˆâ–       | 102/480 [3:22:08<12:13:41, 116.46s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  21%|â–ˆâ–ˆâ–       | 103/480 [3:23:49<11:42:05, 111.74s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 104/480 [3:25:29<11:18:25, 108.26s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 105/480 [3:27:15<11:12:00, 107.52s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 106/480 [3:28:55<10:55:07, 105.10s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 107/480 [3:30:41<10:55:27, 105.44s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  22%|â–ˆâ–ˆâ–Ž       | 108/480 [3:32:19<10:40:35, 103.32s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 109/480 [3:34:14<11:00:20, 106.79s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 110/480 [3:37:09<13:04:37, 127.24s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 111/480 [3:38:53<12:20:23, 120.39s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 112/480 [3:40:29<11:33:19, 113.04s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  24%|â–ˆâ–ˆâ–Ž       | 113/480 [3:42:12<11:12:37, 109.97s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 114/480 [3:43:57<11:01:22, 108.42s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 115/480 [3:45:31<10:32:48, 104.02s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 116/480 [3:47:10<10:22:36, 102.63s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 117/480 [3:48:52<10:20:13, 102.52s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  25%|â–ˆâ–ˆâ–       | 118/480 [3:50:35<10:18:14, 102.47s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  25%|â–ˆâ–ˆâ–       | 119/480 [3:52:14<10:11:42, 101.67s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 01:12:37,981:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 120/480 [3:55:13<12:28:53, 124.82s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 121/480 [3:56:49<11:34:49, 116.13s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 122/480 [3:58:35<11:14:11, 112.99s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 01:18:56,029:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 123/480 [4:00:21<11:00:29, 111.01s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 124/480 [4:02:13<10:59:33, 111.16s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 125/480 [4:03:51<10:34:46, 107.29s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  26%|â–ˆâ–ˆâ–‹       | 126/480 [4:05:33<10:22:55, 105.58s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  26%|â–ˆâ–ˆâ–‹       | 127/480 [4:07:11<10:09:15, 103.56s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 128/480 [4:08:54<10:06:04, 103.31s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 129/480 [4:10:35<10:00:22, 102.63s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 01:32:28,027:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 130/480 [4:13:32<12:08:52, 124.95s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 131/480 [4:15:13<11:24:54, 117.75s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 132/480 [4:16:48<10:42:53, 110.84s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 133/480 [4:18:29<10:24:47, 108.03s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 134/480 [4:20:11<10:11:50, 106.10s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 135/480 [4:21:53<10:02:38, 104.81s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 136/480 [4:23:33<9:53:52, 103.58s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  29%|â–ˆâ–ˆâ–Š       | 137/480 [4:25:13<9:46:00, 102.51s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 138/480 [4:26:59<9:50:09, 103.54s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 139/480 [4:28:39<9:41:35, 102.33s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 140/480 [4:31:29<11:34:43, 122.60s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 141/480 [4:33:11<10:57:58, 116.46s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  30%|â–ˆâ–ˆâ–‰       | 142/480 [4:34:47<10:22:19, 110.47s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  30%|â–ˆâ–ˆâ–‰       | 143/480 [4:36:28<10:03:27, 107.44s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 144/480 [4:38:04<9:42:33, 104.03s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 145/480 [4:39:44<9:34:23, 102.88s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 146/480 [4:41:17<9:16:54, 100.04s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 147/480 [4:42:51<9:05:02, 98.21s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 148/480 [4:44:29<9:01:54, 97.93s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 149/480 [4:46:13<9:10:07, 99.72s/it]
[36m(WorkerDict pid=565198)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=565198)[0m   warnings.warn(
[36m(TaskRunner pid=555874)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆâ–      | 150/480 [4:49:32<11:53:06, 129.66s/it]
[36m(WorkerDict pid=565544)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=565544)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=555874)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆâ–      | 151/480 [4:51:13<11:04:08, 121.12s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 152/480 [4:52:55<10:30:53, 115.41s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 153/480 [4:54:35<10:02:43, 110.59s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 154/480 [4:56:18<9:48:48, 108.37s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 155/480 [4:57:56<9:30:18, 105.29s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 156/480 [4:59:31<9:12:34, 102.33s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 157/480 [5:01:08<9:00:52, 100.47s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 158/480 [5:02:50<9:02:32, 101.10s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 159/480 [5:04:29<8:57:24, 100.45s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 160/480 [5:07:19<10:46:59, 121.31s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 161/480 [5:09:06<10:21:40, 116.93s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 162/480 [5:10:48<9:56:19, 112.51s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 163/480 [5:12:25<9:30:23, 107.96s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 164/480 [5:14:02<9:10:57, 104.61s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 165/480 [5:15:39<8:57:36, 102.40s/it]
[36m(TaskRunner pid=555874)[0m WARNING:2025-11-07 02:35:58,913:Timeout during comparison
[36m(TaskRunner pid=555874)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–      | 166/480 [5:17:23<8:58:20, 102.87s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–      | 167/480 [5:18:58<8:44:19, 100.51s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 168/480 [5:20:39<8:42:42, 100.52s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 169/480 [5:22:14<8:32:43, 98.92s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 170/480 [5:25:00<10:14:43, 118.98s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 171/480 [5:26:36<9:37:53, 112.21s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 172/480 [5:28:14<9:13:46, 107.88s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 173/480 [5:29:53<8:58:06, 105.17s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 174/480 [5:31:27<8:39:46, 101.92s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 175/480 [5:33:07<8:35:29, 101.41s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 176/480 [5:34:47<8:31:02, 100.86s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 177/480 [5:36:24<8:24:03, 99.81s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 178/480 [5:38:04<8:22:22, 99.81s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 179/480 [5:39:44<8:20:13, 99.71s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 180/480 [5:42:37<10:08:36, 121.72s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 181/480 [5:44:16<9:33:40, 115.12s/it] 
[36m(TaskRunner pid=555874)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 182/480 [5:45:52<9:02:44, 109.28s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 183/480 [5:47:33<8:48:10, 106.70s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 184/480 [5:49:07<8:28:37, 103.10s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 185/480 [5:50:52<8:29:12, 103.57s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 186/480 [5:52:31<8:20:27, 102.14s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 187/480 [5:54:07<8:09:25, 100.22s/it]
[36m(TaskRunner pid=555874)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 188/480 [5:55:46<8:06:42, 100.01s/it]
[2025-11-07T03:15:13.002] error: *** JOB 4664817 ON h010 CANCELLED AT 2025-11-07T03:15:13 DUE TO TIME LIMIT ***
