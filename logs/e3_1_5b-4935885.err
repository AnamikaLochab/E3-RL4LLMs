
Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/5.0.5

The following have been reloaded with a version change:
  1) gcc/14.1.0 => gcc/11.4.1

+ NUM_EPISODES=3
+ n_samples_per_prompt=8
+ n_rollout_max=8
+ n_rollout_min=8
+ LR_ACTOR=5e-6
+ entropy_coeff=0
+ n_rollout_update=0
+ enable_temperature_scheduler=False
+ enable_annealing=False
+ TRAIN_DATADIR=./dataset/train_data_10k.parquet
+ VAL_DATADIR=./dataset/valid_data.parquet
+ MODELDIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ PRETRAIN_DIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ SAVE_DIR=../checkpoint/e3_1_5b_dGRPO_0.5_seqq_norm/
+ TENSORBOARD_PATH=../checkpoint/e3_1_5b_dGRPO_0.5_seqq_norm//tensorboard
+ export TENSORBOARD_DIR=../checkpoint/e3_1_5b_dGRPO_0.5_seqq_norm//tensorboard
+ TENSORBOARD_DIR=../checkpoint/e3_1_5b_dGRPO_0.5_seqq_norm//tensorboard
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ python3 -m e3.main_e3 algorithm.adv_estimator=divgrpo data.train_files=./dataset/train_data_10k.parquet data.val_files=./dataset/valid_data.parquet data.train_batch_size=64 data.max_prompt_length=2048 data.max_response_length=6144 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B actor_rollout_ref.actor.optim.lr=5e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=512 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.8 actor_rollout_ref.rollout.n=8 actor_rollout_ref.rollout.n_low=8 actor_rollout_ref.rollout.n_high=8 actor_rollout_ref.rollout.n_update=0 actor_rollout_ref.rollout.temperature=1 actor_rollout_ref.rollout.enable_temperature_scheduler=False actor_rollout_ref.rollout.enable_annealing=False actor_rollout_ref.rollout.max_steps=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 'trainer.logger=[console,tensorboard]' trainer.project_name=GRPO trainer.experiment_name=Qwen trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.default_local_dir=../checkpoint/e3_1_5b_dGRPO_0.5_seqq_norm/ trainer.save_freq=50 trainer.test_freq=10 trainer.total_epochs=3
2025-11-22 17:11:19,278	INFO worker.py:2003 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(TaskRunner pid=3323711)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 17:11:39,360:Waiting for register center actor ZvDFx7_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3327433)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3327432)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(WorkerDict pid=3327432)[0m   warnings.warn(  # warn only once
[36m(WorkerDict pid=3327257)[0m [rank0]:[W1122 17:11:56.121127440 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[36m(WorkerDict pid=3327257)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=3327257)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=3327257)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327257)[0m   warnings.warn(  # warn only once[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m [rank3]:[W1122 17:11:57.298187293 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327433)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3327432)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3327433)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3327433)[0m   warnings.warn(  # warn only once[32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3327433)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3327433)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327433)[0m   warnings.warn(
[36m(WorkerDict pid=3327257)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 2x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:   0%|          | 0/480 [00:00<?, ?it/s]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:   0%|          | 1/480 [01:59<15:55:13, 119.65s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   0%|          | 2/480 [03:58<15:49:31, 119.19s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   1%|          | 3/480 [06:01<16:01:51, 120.99s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   1%|          | 4/480 [07:57<15:42:31, 118.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   1%|          | 5/480 [09:55<15:39:56, 118.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   1%|â–         | 6/480 [11:57<15:47:22, 119.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   1%|â–         | 7/480 [13:59<15:50:25, 120.56s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   2%|â–         | 8/480 [15:58<15:44:48, 120.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   2%|â–         | 9/480 [17:59<15:43:33, 120.20s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   2%|â–         | 10/480 [21:20<18:56:28, 145.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   2%|â–         | 11/480 [23:15<17:43:44, 136.09s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   2%|â–Ž         | 12/480 [25:12<16:54:15, 130.03s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   3%|â–Ž         | 13/480 [27:10<16:25:45, 126.65s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   3%|â–Ž         | 14/480 [29:09<16:05:13, 124.28s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   3%|â–Ž         | 15/480 [31:05<15:43:34, 121.75s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   3%|â–Ž         | 16/480 [33:04<15:34:21, 120.82s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   4%|â–Ž         | 17/480 [35:02<15:26:44, 120.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   4%|â–         | 18/480 [36:59<15:16:43, 119.06s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   4%|â–         | 19/480 [38:58<15:14:03, 118.97s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   4%|â–         | 20/480 [42:18<18:18:31, 143.29s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 17:57:43,941:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:   4%|â–         | 21/480 [44:21<17:31:34, 137.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   5%|â–         | 22/480 [46:21<16:47:47, 132.03s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   5%|â–         | 23/480 [48:19<16:13:27, 127.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   5%|â–Œ         | 24/480 [50:14<15:41:42, 123.91s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   5%|â–Œ         | 25/480 [52:08<15:17:21, 120.97s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   5%|â–Œ         | 26/480 [54:02<15:01:21, 119.12s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   6%|â–Œ         | 27/480 [56:02<15:01:22, 119.39s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   6%|â–Œ         | 28/480 [57:53<14:40:25, 116.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   6%|â–Œ         | 29/480 [59:51<14:40:27, 117.13s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   6%|â–‹         | 30/480 [1:03:00<17:20:33, 138.74s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   6%|â–‹         | 31/480 [1:04:47<16:05:28, 129.02s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   7%|â–‹         | 32/480 [1:06:33<15:12:28, 122.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   7%|â–‹         | 33/480 [1:08:32<15:02:12, 121.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   7%|â–‹         | 34/480 [1:10:24<14:41:50, 118.63s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   7%|â–‹         | 35/480 [1:12:16<14:24:48, 116.60s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   8%|â–Š         | 36/480 [1:14:07<14:10:22, 114.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   8%|â–Š         | 37/480 [1:16:01<14:05:23, 114.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   8%|â–Š         | 38/480 [1:18:00<14:14:13, 115.96s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   8%|â–Š         | 39/480 [1:19:49<13:57:22, 113.93s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   8%|â–Š         | 40/480 [1:22:58<16:40:58, 136.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   9%|â–Š         | 41/480 [1:24:48<15:38:50, 128.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   9%|â–‰         | 42/480 [1:26:43<15:09:10, 124.54s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   9%|â–‰         | 43/480 [1:28:34<14:35:49, 120.25s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   9%|â–‰         | 44/480 [1:30:25<14:13:30, 117.45s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:   9%|â–‰         | 45/480 [1:32:12<13:49:10, 114.37s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  10%|â–‰         | 46/480 [1:34:05<13:43:51, 113.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  10%|â–‰         | 47/480 [1:35:56<13:36:30, 113.14s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  10%|â–ˆ         | 48/480 [1:37:49<13:34:28, 113.12s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  10%|â–ˆ         | 49/480 [1:39:47<13:42:02, 114.44s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  10%|â–ˆ         | 50/480 [1:43:07<16:45:47, 140.34s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  11%|â–ˆ         | 51/480 [1:44:58<15:39:26, 131.39s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  11%|â–ˆ         | 52/480 [1:46:50<14:55:14, 125.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  11%|â–ˆ         | 53/480 [1:48:36<14:12:27, 119.78s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  11%|â–ˆâ–        | 54/480 [1:50:29<13:55:08, 117.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  11%|â–ˆâ–        | 55/480 [1:52:18<13:35:23, 115.11s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  12%|â–ˆâ–        | 56/480 [1:54:08<13:22:13, 113.52s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  12%|â–ˆâ–        | 57/480 [1:56:00<13:17:06, 113.07s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  12%|â–ˆâ–        | 58/480 [1:57:45<12:58:21, 110.67s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  12%|â–ˆâ–        | 59/480 [1:59:30<12:44:29, 108.95s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 19:14:46,299:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  12%|â–ˆâ–Ž        | 60/480 [2:02:33<15:18:49, 131.26s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  13%|â–ˆâ–Ž        | 61/480 [2:04:23<14:31:45, 124.83s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  13%|â–ˆâ–Ž        | 62/480 [2:06:11<13:53:44, 119.68s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  13%|â–ˆâ–Ž        | 63/480 [2:07:57<13:23:45, 115.65s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  13%|â–ˆâ–Ž        | 64/480 [2:09:48<13:12:30, 114.30s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  14%|â–ˆâ–Ž        | 65/480 [2:11:34<12:52:48, 111.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  14%|â–ˆâ–        | 66/480 [2:13:25<12:50:19, 111.64s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  14%|â–ˆâ–        | 67/480 [2:15:11<12:36:59, 109.98s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  14%|â–ˆâ–        | 68/480 [2:16:57<12:26:21, 108.69s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  14%|â–ˆâ–        | 69/480 [2:18:48<12:29:00, 109.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  15%|â–ˆâ–        | 70/480 [2:21:51<14:59:01, 131.56s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  15%|â–ˆâ–        | 71/480 [2:23:39<14:09:15, 124.59s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  15%|â–ˆâ–Œ        | 72/480 [2:25:27<13:32:29, 119.48s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  15%|â–ˆâ–Œ        | 73/480 [2:27:14<13:04:29, 115.65s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  15%|â–ˆâ–Œ        | 74/480 [2:29:05<12:53:23, 114.29s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  16%|â–ˆâ–Œ        | 75/480 [2:30:52<12:37:53, 112.28s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  16%|â–ˆâ–Œ        | 76/480 [2:32:44<12:34:01, 111.98s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  16%|â–ˆâ–Œ        | 77/480 [2:34:33<12:26:07, 111.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  16%|â–ˆâ–‹        | 78/480 [2:36:23<12:23:21, 110.95s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  16%|â–ˆâ–‹        | 79/480 [2:38:13<12:18:35, 110.51s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  17%|â–ˆâ–‹        | 80/480 [2:41:13<14:36:05, 131.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  17%|â–ˆâ–‹        | 81/480 [2:43:02<13:49:38, 124.76s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  17%|â–ˆâ–‹        | 82/480 [2:44:50<13:14:07, 119.72s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  17%|â–ˆâ–‹        | 83/480 [2:46:40<12:52:10, 116.70s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  18%|â–ˆâ–Š        | 84/480 [2:48:29<12:36:06, 114.56s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  18%|â–ˆâ–Š        | 85/480 [2:50:20<12:26:02, 113.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  18%|â–ˆâ–Š        | 86/480 [2:52:08<12:13:16, 111.67s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  18%|â–ˆâ–Š        | 87/480 [2:53:58<12:07:52, 111.13s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  18%|â–ˆâ–Š        | 88/480 [2:55:49<12:05:45, 111.09s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  19%|â–ˆâ–Š        | 89/480 [2:57:35<11:54:43, 109.68s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  19%|â–ˆâ–‰        | 90/480 [3:00:35<14:09:36, 130.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  19%|â–ˆâ–‰        | 91/480 [3:02:21<13:19:04, 123.25s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  19%|â–ˆâ–‰        | 92/480 [3:04:04<12:39:19, 117.42s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  19%|â–ˆâ–‰        | 93/480 [3:05:52<12:17:29, 114.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  20%|â–ˆâ–‰        | 94/480 [3:07:41<12:06:20, 112.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  20%|â–ˆâ–‰        | 95/480 [3:09:26<11:49:36, 110.59s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  20%|â–ˆâ–ˆ        | 96/480 [3:11:19<11:52:43, 111.36s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 20:26:36,166:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  20%|â–ˆâ–ˆ        | 97/480 [3:13:10<11:49:36, 111.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  20%|â–ˆâ–ˆ        | 98/480 [3:14:51<11:28:12, 108.09s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  21%|â–ˆâ–ˆ        | 99/480 [3:16:45<11:37:39, 109.87s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 20:32:00,615:Timeout during comparison
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(WorkerDict pid=3327257)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is bfloat16. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=3327257)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is bfloat16. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(TaskRunner pid=3323711)[0m Training Progress:  21%|â–ˆâ–ˆ        | 100/480 [3:20:11<14:38:47, 138.76s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  21%|â–ˆâ–ˆ        | 101/480 [3:21:57<13:34:17, 128.91s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  21%|â–ˆâ–ˆâ–       | 102/480 [3:23:45<12:51:22, 122.44s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  21%|â–ˆâ–ˆâ–       | 103/480 [3:25:28<12:13:01, 116.66s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 104/480 [3:27:12<11:47:10, 112.85s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 105/480 [3:28:59<11:35:17, 111.25s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 106/480 [3:30:41<11:16:29, 108.53s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  22%|â–ˆâ–ˆâ–       | 107/480 [3:32:32<11:18:23, 109.12s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 20:47:45,573:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  22%|â–ˆâ–ˆâ–Ž       | 108/480 [3:34:18<11:11:22, 108.29s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 109/480 [3:36:15<11:26:07, 110.96s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 110/480 [3:39:16<13:33:38, 131.94s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 111/480 [3:41:03<12:44:39, 124.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  23%|â–ˆâ–ˆâ–Ž       | 112/480 [3:42:43<11:58:53, 117.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  24%|â–ˆâ–ˆâ–Ž       | 113/480 [3:44:30<11:38:11, 114.15s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 114/480 [3:46:21<11:29:41, 113.06s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 115/480 [3:47:58<10:58:48, 108.30s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 116/480 [3:49:42<10:48:48, 106.95s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  24%|â–ˆâ–ˆâ–       | 117/480 [3:51:28<10:44:32, 106.53s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  25%|â–ˆâ–ˆâ–       | 118/480 [3:53:14<10:42:01, 106.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  25%|â–ˆâ–ˆâ–       | 119/480 [3:54:59<10:38:26, 106.11s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 120/480 [3:57:59<12:49:14, 128.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 121/480 [3:59:38<11:55:04, 119.51s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  25%|â–ˆâ–ˆâ–Œ       | 122/480 [4:01:27<11:34:59, 116.48s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 123/480 [4:03:10<11:08:57, 112.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 124/480 [4:05:05<11:10:46, 113.05s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  26%|â–ˆâ–ˆâ–Œ       | 125/480 [4:06:47<10:49:03, 109.70s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  26%|â–ˆâ–ˆâ–‹       | 126/480 [4:08:32<10:39:09, 108.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  26%|â–ˆâ–ˆâ–‹       | 127/480 [4:10:13<10:23:48, 106.03s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 128/480 [4:11:55<10:14:53, 104.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 129/480 [4:13:40<10:13:29, 104.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 130/480 [4:16:36<12:17:22, 126.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  27%|â–ˆâ–ˆâ–‹       | 131/480 [4:18:20<11:35:35, 119.59s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 132/480 [4:19:58<10:56:22, 113.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 133/480 [4:21:42<10:37:43, 110.27s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 134/480 [4:23:24<10:21:28, 107.77s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 135/480 [4:25:08<10:14:10, 106.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  28%|â–ˆâ–ˆâ–Š       | 136/480 [4:26:51<10:04:52, 105.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  29%|â–ˆâ–ˆâ–Š       | 137/480 [4:28:32<9:56:00, 104.26s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 138/480 [4:30:16<9:54:02, 104.22s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 139/480 [4:31:57<9:46:20, 103.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 140/480 [4:34:50<11:43:37, 124.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  29%|â–ˆâ–ˆâ–‰       | 141/480 [4:36:31<11:03:07, 117.37s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  30%|â–ˆâ–ˆâ–‰       | 142/480 [4:38:10<10:28:50, 111.63s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  30%|â–ˆâ–ˆâ–‰       | 143/480 [4:39:53<10:12:55, 109.13s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 144/480 [4:41:32<9:54:08, 106.10s/it] 
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 21:56:44,559:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 145/480 [4:43:17<9:50:50, 105.82s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 146/480 [4:44:50<9:27:57, 102.03s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 147/480 [4:46:26<9:16:16, 100.23s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 148/480 [4:48:03<9:09:01, 99.22s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆ       | 149/480 [4:49:48<9:16:01, 100.79s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆâ–      | 150/480 [4:52:58<11:41:28, 127.54s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  31%|â–ˆâ–ˆâ–ˆâ–      | 151/480 [4:54:39<10:56:44, 119.77s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 152/480 [4:56:22<10:26:13, 114.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 153/480 [4:57:59<9:56:48, 109.51s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 154/480 [4:59:43<9:45:21, 107.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–      | 155/480 [5:01:24<9:32:07, 105.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 156/480 [5:02:59<9:13:49, 102.56s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 157/480 [5:04:36<9:03:24, 100.94s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 158/480 [5:06:19<9:04:17, 101.42s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 159/480 [5:07:59<9:00:28, 101.02s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 22:23:16,324:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 160/480 [5:10:59<11:05:08, 124.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 161/480 [5:12:46<10:34:26, 119.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 162/480 [5:14:29<10:06:59, 114.53s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 163/480 [5:16:07<9:39:24, 109.67s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 164/480 [5:17:45<9:18:17, 106.01s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 165/480 [5:19:23<9:03:39, 103.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–      | 166/480 [5:21:04<8:58:58, 102.99s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–      | 167/480 [5:22:39<8:45:00, 100.64s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 168/480 [5:24:21<8:44:38, 100.89s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 169/480 [5:26:00<8:39:25, 100.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 170/480 [5:28:52<10:29:54, 121.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 171/480 [5:30:33<9:54:48, 115.50s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 172/480 [5:32:11<9:27:02, 110.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 173/480 [5:33:50<9:06:51, 106.88s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 174/480 [5:35:26<8:48:03, 103.54s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 175/480 [5:37:06<8:41:40, 102.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 176/480 [5:38:49<8:41:03, 102.84s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 177/480 [5:40:30<8:35:21, 102.05s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 178/480 [5:42:14<8:36:23, 102.60s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 179/480 [5:43:54<8:31:54, 102.04s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 180/480 [5:46:53<10:25:20, 125.07s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 23:02:08,908:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 181/480 [5:48:41<9:56:59, 119.80s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 182/480 [5:50:21<9:26:32, 114.07s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 183/480 [5:52:04<9:08:08, 110.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 184/480 [5:53:42<8:46:43, 106.77s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 185/480 [5:55:27<8:43:20, 106.44s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 186/480 [5:57:09<8:34:40, 105.03s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 187/480 [5:58:48<8:24:22, 103.28s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 188/480 [6:00:30<8:20:50, 102.91s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 189/480 [6:02:13<8:19:02, 102.89s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 190/480 [6:05:08<10:00:43, 124.29s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 191/480 [6:06:44<9:17:49, 115.81s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 192/480 [6:08:20<8:48:01, 110.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 193/480 [6:09:59<8:31:00, 106.83s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 194/480 [6:11:38<8:17:51, 104.45s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 195/480 [6:13:18<8:09:53, 103.14s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 196/480 [6:15:00<8:05:30, 102.57s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 197/480 [6:16:37<7:55:43, 100.86s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 198/480 [6:18:17<7:53:45, 100.80s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 199/480 [6:19:55<7:48:15, 99.98s/it] 
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 200/480 [6:23:07<9:54:37, 127.42s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 201/480 [6:24:48<9:15:54, 119.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 202/480 [6:26:27<8:45:52, 113.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 203/480 [6:28:05<8:22:42, 108.89s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 204/480 [6:29:44<8:06:33, 105.77s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 205/480 [6:31:23<7:56:13, 103.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 206/480 [6:33:06<7:52:29, 103.47s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 207/480 [6:34:49<7:50:15, 103.35s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 208/480 [6:36:23<7:35:17, 100.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 209/480 [6:38:02<7:32:35, 100.20s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 210/480 [6:41:00<9:15:19, 123.41s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 23:56:13,937:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-22 23:56:20,369:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 211/480 [6:42:52<8:58:05, 120.02s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212/480 [6:44:34<8:32:23, 114.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 213/480 [6:46:18<8:16:10, 111.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 214/480 [6:47:57<7:57:37, 107.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 215/480 [6:49:35<7:42:52, 104.80s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 216/480 [6:51:14<7:33:49, 103.14s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 217/480 [6:52:53<7:25:39, 101.67s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 218/480 [6:54:29<7:16:43, 100.01s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 219/480 [6:56:13<7:20:39, 101.30s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 220/480 [6:59:04<8:49:17, 122.14s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 221/480 [7:00:43<8:17:31, 115.26s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 222/480 [7:02:24<7:57:18, 111.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 223/480 [7:03:56<7:30:30, 105.18s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 224/480 [7:05:36<7:22:27, 103.70s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 225/480 [7:07:12<7:10:56, 101.40s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 226/480 [7:08:49<7:04:09, 100.20s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 227/480 [7:10:32<7:05:53, 101.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 228/480 [7:12:13<7:03:43, 100.89s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 229/480 [7:13:56<7:04:45, 101.53s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 230/480 [7:16:50<8:34:01, 123.37s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 231/480 [7:18:27<7:58:21, 115.27s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 232/480 [7:20:08<7:38:56, 111.04s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 233/480 [7:21:49<7:24:55, 108.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 234/480 [7:23:28<7:11:21, 105.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 235/480 [7:25:06<7:01:39, 103.26s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 236/480 [7:26:46<6:55:33, 102.18s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 237/480 [7:28:23<6:47:46, 100.69s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 238/480 [7:30:08<6:50:52, 101.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 239/480 [7:31:47<6:46:30, 101.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 240/480 [7:34:44<8:15:07, 123.78s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 241/480 [7:36:31<7:53:21, 118.83s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 242/480 [7:38:11<7:29:14, 113.26s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 243/480 [7:39:54<7:14:16, 109.94s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 244/480 [7:41:35<7:02:34, 107.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 245/480 [7:43:17<6:53:58, 105.69s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 246/480 [7:44:55<6:43:35, 103.49s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 247/480 [7:46:36<6:38:42, 102.67s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 248/480 [7:48:18<6:36:22, 102.51s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 249/480 [7:49:59<6:32:47, 102.03s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 250/480 [7:53:08<8:10:52, 128.06s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 251/480 [7:54:45<7:34:03, 118.97s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 252/480 [7:56:22<7:06:52, 112.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 253/480 [7:57:59<6:47:07, 107.61s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:13:08,730:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 254/480 [7:59:41<6:38:31, 105.80s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 255/480 [8:01:16<6:25:36, 102.83s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 256/480 [8:02:58<6:22:16, 102.40s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 257/480 [8:04:32<6:11:41, 100.01s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 258/480 [8:06:13<6:10:43, 100.20s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:21:22,741:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:21:28,487:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 259/480 [8:07:57<6:13:44, 101.47s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 260/480 [8:10:45<7:24:51, 121.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261/480 [8:12:24<6:58:17, 114.60s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262/480 [8:14:03<6:40:04, 110.11s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 263/480 [8:15:44<6:27:20, 107.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 264/480 [8:17:26<6:20:57, 105.82s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 265/480 [8:19:02<6:08:23, 102.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 266/480 [8:20:39<5:59:47, 100.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 267/480 [8:22:14<5:52:24, 99.27s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 268/480 [8:23:46<5:42:27, 96.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 269/480 [8:25:22<5:40:22, 96.79s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:40:32,848:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:40:38,760:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:40:44,673:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 270/480 [8:28:27<7:11:02, 123.16s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 271/480 [8:30:04<6:41:44, 115.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 272/480 [8:31:41<6:20:35, 109.78s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 273/480 [8:33:16<6:03:27, 105.35s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 274/480 [8:34:55<5:55:16, 103.48s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 275/480 [8:36:27<5:41:51, 100.06s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 276/480 [8:38:06<5:39:02, 99.72s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 277/480 [8:39:44<5:36:19, 99.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 278/480 [8:41:19<5:30:17, 98.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 279/480 [8:42:55<5:26:11, 97.37s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 01:59:33,752:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 280/480 [8:45:46<6:37:38, 119.29s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 281/480 [8:47:14<6:05:13, 110.12s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 282/480 [8:48:48<5:47:37, 105.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 283/480 [8:50:24<5:36:34, 102.51s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 02:05:34,309:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 284/480 [8:52:08<5:36:14, 102.93s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 285/480 [8:53:44<5:27:06, 100.65s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 286/480 [8:55:17<5:18:23, 98.47s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 287/480 [8:56:52<5:13:37, 97.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 288/480 [8:58:28<5:09:59, 96.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 289/480 [9:00:02<5:06:15, 96.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 290/480 [9:02:50<6:12:52, 117.75s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 291/480 [9:04:24<5:47:46, 110.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 292/480 [9:05:57<5:29:59, 105.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 293/480 [9:07:29<5:15:17, 101.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 294/480 [9:09:02<5:06:50, 98.98s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 295/480 [9:10:37<5:01:25, 97.76s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 296/480 [9:12:16<5:00:25, 97.97s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 297/480 [9:13:47<4:52:25, 95.88s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 298/480 [9:15:21<4:49:12, 95.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 299/480 [9:16:53<4:44:40, 94.37s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 300/480 [9:19:54<6:01:13, 120.41s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 301/480 [9:21:28<5:35:46, 112.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 302/480 [9:23:03<5:17:36, 107.06s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 303/480 [9:24:41<5:08:01, 104.42s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 304/480 [9:26:18<4:59:44, 102.19s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 305/480 [9:27:50<4:49:40, 99.32s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 306/480 [9:29:25<4:43:57, 97.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 307/480 [9:30:57<4:37:29, 96.24s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308/480 [9:32:32<4:34:41, 95.82s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309/480 [9:34:12<4:36:18, 96.95s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310/480 [9:36:59<5:34:02, 117.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 311/480 [9:38:35<5:13:57, 111.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 312/480 [9:40:13<5:00:30, 107.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 313/480 [9:41:48<4:48:25, 103.63s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 314/480 [9:43:26<4:42:08, 101.98s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 315/480 [9:45:06<4:38:38, 101.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 316/480 [9:46:47<4:37:04, 101.37s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:01:55,088:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:02:06,017:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 317/480 [9:48:35<4:40:22, 103.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 318/480 [9:50:11<4:33:10, 101.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 319/480 [9:51:49<4:28:50, 100.19s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:07:00,644:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 320/480 [9:54:42<5:25:25, 122.04s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 321/480 [9:56:21<5:04:52, 115.05s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:11:28,082:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:11:34,252:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 322/480 [9:58:04<4:53:23, 111.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 323/480 [9:59:39<4:38:58, 106.61s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 324/480 [10:01:22<4:34:00, 105.39s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 325/480 [10:03:01<4:27:41, 103.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 326/480 [10:04:41<4:23:15, 102.57s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 327/480 [10:06:22<4:20:31, 102.16s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 328/480 [10:08:02<4:17:01, 101.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 329/480 [10:09:37<4:10:09, 99.40s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 330/480 [10:12:27<5:01:51, 120.74s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 331/480 [10:14:02<4:40:40, 113.02s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 332/480 [10:15:37<4:25:13, 107.53s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 333/480 [10:17:19<4:19:10, 105.79s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 334/480 [10:18:57<4:12:03, 103.58s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 335/480 [10:20:33<4:04:30, 101.18s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 336/480 [10:22:09<3:59:13, 99.68s/it] 
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:37:20,211:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 337/480 [10:23:53<4:00:56, 101.09s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 338/480 [10:25:35<3:59:20, 101.13s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 339/480 [10:27:10<3:53:17, 99.28s/it] 
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:43:53,683:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 340/480 [10:30:06<4:45:36, 122.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 341/480 [10:31:46<4:28:04, 115.72s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 342/480 [10:33:24<4:13:57, 110.41s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 343/480 [10:35:04<4:05:05, 107.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 344/480 [10:36:39<3:54:27, 103.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 345/480 [10:38:20<3:51:16, 102.79s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 346/480 [10:39:58<3:46:08, 101.26s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 03:55:10,825:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 347/480 [10:41:42<3:46:28, 102.17s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 348/480 [10:43:24<3:44:37, 102.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 349/480 [10:44:59<3:38:16, 99.97s/it] 
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 350/480 [10:48:11<4:36:49, 127.77s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 351/480 [10:49:54<4:18:27, 120.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 352/480 [10:51:36<4:04:57, 114.83s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 353/480 [10:53:21<3:56:22, 111.67s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 354/480 [10:55:03<3:48:30, 108.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 355/480 [10:56:45<3:42:54, 107.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 356/480 [10:58:24<3:35:36, 104.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/480 [11:00:05<3:32:10, 103.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358/480 [11:01:43<3:26:52, 101.74s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 359/480 [11:03:30<3:28:21, 103.32s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 360/480 [11:06:23<4:08:27, 124.23s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 361/480 [11:08:04<3:52:34, 117.26s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 362/480 [11:09:46<3:41:58, 112.87s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 363/480 [11:11:27<3:32:40, 109.07s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 364/480 [11:13:04<3:24:22, 105.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 365/480 [11:14:43<3:18:24, 103.52s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 366/480 [11:16:19<3:12:43, 101.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 367/480 [11:17:56<3:08:14, 99.95s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 368/480 [11:19:38<3:07:40, 100.54s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 369/480 [11:21:14<3:03:36, 99.25s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 370/480 [11:24:11<3:44:43, 122.57s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 371/480 [11:25:51<3:30:19, 115.78s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 372/480 [11:27:33<3:20:46, 111.54s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 373/480 [11:29:17<3:14:49, 109.25s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 374/480 [11:30:53<3:06:06, 105.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 375/480 [11:32:35<3:02:43, 104.42s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 376/480 [11:34:12<2:57:07, 102.19s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 377/480 [11:35:52<2:54:10, 101.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 378/480 [11:37:32<2:51:37, 100.96s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 379/480 [11:39:09<2:48:16, 99.97s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 380/480 [11:42:04<3:24:09, 122.49s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 381/480 [11:43:45<3:11:14, 115.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 382/480 [11:45:23<3:00:49, 110.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 383/480 [11:47:05<2:54:32, 107.96s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 384/480 [11:48:54<2:53:14, 108.28s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 385/480 [11:50:34<2:47:23, 105.72s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 386/480 [11:52:13<2:42:29, 103.71s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 387/480 [11:53:54<2:39:31, 102.92s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 388/480 [11:55:31<2:35:23, 101.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 389/480 [11:57:12<2:33:18, 101.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 390/480 [12:00:04<3:03:37, 122.42s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 391/480 [12:01:44<2:51:23, 115.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 392/480 [12:03:18<2:40:00, 109.10s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 393/480 [12:04:51<2:31:16, 104.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 394/480 [12:06:29<2:26:59, 102.55s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 395/480 [12:08:12<2:25:14, 102.52s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 396/480 [12:09:47<2:20:36, 100.44s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 397/480 [12:11:25<2:17:55, 99.71s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 398/480 [12:13:00<2:14:10, 98.18s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 399/480 [12:14:36<2:11:50, 97.66s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 400/480 [12:17:41<2:45:00, 123.76s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 401/480 [12:19:20<2:33:19, 116.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 402/480 [12:21:04<2:26:13, 112.48s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 403/480 [12:22:42<2:18:45, 108.12s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 404/480 [12:24:19<2:12:52, 104.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 405/480 [12:25:49<2:05:40, 100.54s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 406/480 [12:27:26<2:02:40, 99.47s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407/480 [12:29:06<2:01:02, 99.49s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 408/480 [12:30:45<1:59:14, 99.37s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 409/480 [12:32:20<1:55:59, 98.02s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 410/480 [12:35:15<2:21:24, 121.20s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 411/480 [12:36:52<2:10:50, 113.78s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 412/480 [12:38:27<2:02:33, 108.14s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 413/480 [12:40:04<1:57:05, 104.86s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 414/480 [12:41:41<1:52:53, 102.63s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 415/480 [12:43:11<1:47:06, 98.88s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 416/480 [12:44:50<1:45:15, 98.68s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 417/480 [12:46:29<1:43:42, 98.77s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 418/480 [12:48:06<1:41:48, 98.52s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 419/480 [12:49:44<1:39:59, 98.36s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 420/480 [12:52:41<2:01:43, 121.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 421/480 [12:54:17<1:52:18, 114.21s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 422/480 [12:56:03<1:47:52, 111.60s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 423/480 [12:57:41<1:42:14, 107.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 424/480 [12:59:21<1:38:20, 105.37s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 06:14:32,886:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 425/480 [13:01:07<1:36:39, 105.45s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 426/480 [13:02:46<1:33:06, 103.46s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 427/480 [13:04:25<1:30:15, 102.18s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 428/480 [13:06:07<1:28:36, 102.24s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 429/480 [13:07:45<1:25:48, 100.94s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 430/480 [13:10:37<1:41:50, 122.20s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 431/480 [13:12:16<1:33:58, 115.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 432/480 [13:13:53<1:27:47, 109.75s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 433/480 [13:15:38<1:24:51, 108.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 434/480 [13:17:15<1:20:28, 104.97s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 435/480 [13:18:50<1:16:29, 102.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 436/480 [13:20:28<1:13:59, 100.90s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 437/480 [13:22:09<1:12:11, 100.73s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 438/480 [13:23:47<1:09:54, 99.87s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 439/480 [13:25:24<1:07:48, 99.23s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 440/480 [13:28:14<1:20:15, 120.38s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 441/480 [13:29:52<1:13:48, 113.56s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 442/480 [13:31:32<1:09:19, 109.47s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 443/480 [13:33:10<1:05:21, 106.00s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 444/480 [13:34:48<1:02:09, 103.59s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 445/480 [13:36:25<59:21, 101.77s/it]  
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 06:51:33,601:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 06:51:39,236:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 06:51:44,978:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 446/480 [13:38:14<58:52, 103.91s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 447/480 [13:39:47<55:22, 100.68s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 448/480 [13:41:20<52:25, 98.30s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 449/480 [13:42:59<50:55, 98.57s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 450/480 [13:46:04<1:02:13, 124.45s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3323711)[0m Training Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 451/480 [13:47:40<55:59, 115.86s/it]  
[36m(TaskRunner pid=3323711)[0m Training Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 452/480 [13:49:11<50:41, 108.62s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 453/480 [13:50:50<47:32, 105.66s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 454/480 [13:52:25<44:18, 102.27s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 455/480 [13:53:55<41:11, 98.85s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 456/480 [13:55:29<38:56, 97.36s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 457/480 [13:57:10<37:43, 98.43s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 458/480 [13:58:47<35:54, 97.94s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 459/480 [14:00:21<33:52, 96.81s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 460/480 [14:03:10<39:26, 118.33s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 461/480 [14:04:48<35:34, 112.34s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 462/480 [14:06:22<32:02, 106.83s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 07:21:31,342:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 463/480 [14:08:02<29:40, 104.76s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 464/480 [14:09:36<27:03, 101.44s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 465/480 [14:11:12<24:58, 99.89s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 466/480 [14:12:48<23:00, 98.64s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 467/480 [14:14:23<21:07, 97.50s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 468/480 [14:16:02<19:36, 98.05s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 469/480 [14:17:37<17:47, 97.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 470/480 [14:20:31<20:03, 120.36s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 471/480 [14:22:07<16:56, 112.91s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 472/480 [14:23:45<14:27, 108.40s/it]
[36m(TaskRunner pid=3323711)[0m WARNING:2025-11-23 07:38:55,921:Timeout during comparison
[36m(TaskRunner pid=3323711)[0m Training Progress:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 473/480 [14:25:26<12:24, 106.35s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 474/480 [14:27:02<10:18, 103.08s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 475/480 [14:28:35<08:21, 100.27s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 476/480 [14:30:14<06:39, 99.75s/it] 
[36m(TaskRunner pid=3323711)[0m Training Progress:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 477/480 [14:31:49<04:54, 98.30s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 478/480 [14:33:31<03:18, 99.37s/it]
[36m(TaskRunner pid=3323711)[0m Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 479/480 [14:35:14<01:40, 100.58s/it]
[36m(WorkerDict pid=3327257)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3327257)[0m   warnings.warn(
[36m(TaskRunner pid=3323711)[0m Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 479/480 [14:38:25<01:50, 110.03s/it]
[36m(WorkerDict pid=3327434)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3327434)[0m   warnings.warn([32m [repeated 3x across cluster][0m
