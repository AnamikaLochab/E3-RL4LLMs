The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) xalt/3.1.4
+ NUM_EPISODES=3
+ n_samples_per_prompt=8
+ n_rollout_max=8
+ n_rollout_min=8
+ LR_ACTOR=5e-6
+ entropy_coeff=0
+ n_rollout_update=0
+ enable_temperature_scheduler=False
+ enable_annealing=False
+ TRAIN_DATADIR=./dataset/train_data_10k.parquet
+ VAL_DATADIR=./dataset/valid_data.parquet
+ MODELDIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ PRETRAIN_DIR=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
+ SAVE_DIR=../checkpoint/e3_1_5b_dGRPO_1.0_a1/
+ TENSORBOARD_PATH=../checkpoint/e3_1_5b_dGRPO_1.0_a1//tensorboard
+ export TENSORBOARD_DIR=../checkpoint/e3_1_5b_dGRPO_1.0_a1//tensorboard
+ TENSORBOARD_DIR=../checkpoint/e3_1_5b_dGRPO_1.0_a1//tensorboard
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ python3 -m e3.main_e3 algorithm.adv_estimator=divgrpo data.train_files=./dataset/train_data_10k.parquet data.val_files=./dataset/valid_data.parquet data.train_batch_size=64 data.max_prompt_length=2048 data.max_response_length=6144 data.filter_overlong_prompts=True data.truncation=error actor_rollout_ref.model.path=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B actor_rollout_ref.actor.optim.lr=5e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=512 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.8 actor_rollout_ref.rollout.n=8 actor_rollout_ref.rollout.n_low=8 actor_rollout_ref.rollout.n_high=8 actor_rollout_ref.rollout.n_update=0 actor_rollout_ref.rollout.temperature=1 actor_rollout_ref.rollout.enable_temperature_scheduler=False actor_rollout_ref.rollout.enable_annealing=False actor_rollout_ref.rollout.max_steps=4 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.use_kl_in_reward=False trainer.critic_warmup=0 'trainer.logger=[console,tensorboard]' trainer.project_name=GRPO trainer.experiment_name=Qwen trainer.n_gpus_per_node=4 trainer.nnodes=1 trainer.default_local_dir=../checkpoint/e3_1_5b_dGRPO_1.0_a1/ trainer.save_freq=50 trainer.test_freq=10 trainer.total_epochs=3
2025-12-19 15:17:42,074	INFO worker.py:2003 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(TaskRunner pid=3098789)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3098789)[0m WARNING:2025-12-19 15:18:01,734:Waiting for register center actor 3t2cHj_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3102519)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3102519)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[36m(WorkerDict pid=3102519)[0m   warnings.warn(  # warn only once
[36m(WorkerDict pid=3102342)[0m [rank0]:[W1219 15:18:19.145261613 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[36m(WorkerDict pid=3102519)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=3102519)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=3102342)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3102342)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3102342)[0m   warnings.warn(  # warn only once[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3102520)[0m [rank3]:[W1219 15:18:19.372683037 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3102520)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3102342)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3102520)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. [32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3102520)[0m   warnings.warn(  # warn only once[32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3102519)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=3102520)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3102520)[0m   warnings.warn(
[36m(WorkerDict pid=3102520)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 2x across cluster][0m
[36m(TaskRunner pid=3098789)[0m Training Progress:   0%|          | 0/480 [00:00<?, ?it/s]
[36m(WorkerDict pid=3102517)[0m /scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3102517)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3098789)[0m /scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/trainer/ppo/core_algos.py:744: RuntimeWarning: divide by zero encountered in log
[36m(TaskRunner pid=3098789)[0m   print(f"    Entropy H(q):   {Hq.item():.4f} (Max possible: {np.log(corr_mask.sum().item()):.4f})")
Error executing job with overrides: ['algorithm.adv_estimator=divgrpo', 'data.train_files=./dataset/train_data_10k.parquet', 'data.val_files=./dataset/valid_data.parquet', 'data.train_batch_size=64', 'data.max_prompt_length=2048', 'data.max_response_length=6144', 'data.filter_overlong_prompts=True', 'data.truncation=error', 'actor_rollout_ref.model.path=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'actor_rollout_ref.actor.optim.lr=5e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=512', 'actor_rollout_ref.actor.use_dynamic_bsz=True', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.8', 'actor_rollout_ref.rollout.n=8', 'actor_rollout_ref.rollout.n_low=8', 'actor_rollout_ref.rollout.n_high=8', 'actor_rollout_ref.rollout.n_update=0', 'actor_rollout_ref.rollout.temperature=1', 'actor_rollout_ref.rollout.enable_temperature_scheduler=False', 'actor_rollout_ref.rollout.enable_annealing=False', 'actor_rollout_ref.rollout.max_steps=4', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.use_kl_in_reward=False', 'trainer.critic_warmup=0', 'trainer.logger=[console,tensorboard]', 'trainer.project_name=GRPO', 'trainer.experiment_name=Qwen', 'trainer.n_gpus_per_node=4', 'trainer.nnodes=1', 'trainer.default_local_dir=../checkpoint/e3_1_5b_dGRPO_1.0_a1/', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=3']
Traceback (most recent call last):
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/main_e3.py", line 183, in <module>
    main()
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/main_e3.py", line 57, in main
    run_ppo(config)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/main_e3.py", line 75, in run_ppo
    ray.get(runner.run.remote(config))
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
  File "/scratch/gautschi/alochab/conda_envs/e3/lib/python3.10/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): [36mray::TaskRunner.run()[39m (pid=3098789, ip=172.18.49.219, actor_id=6d073ceeec2f7ace66e24a1801000000, repr=<main_e3.TaskRunner object at 0x14675c645750>)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/main_e3.py", line 179, in run
    trainer.fit()
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/trainer/ray_trainer_cov_base.py", line 1480, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(RuntimeError): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3102517, ip=172.18.49.219, actor_id=4376ae237dbe4fe78735e73801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14efc160ae60>)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/ray/base.py", line 466, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/base/decorator.py", line 501, in inner
    return func(*args, **kwargs)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/worker/fsdp_work.py", line 464, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 78, in f
    return self.log(decorated_function, *args, **kwargs)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 88, in log
    output = func(*args, **kwargs)
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/workers/actor/dp_actor.py", line 392, in update_policy
    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
  File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/trainer/ppo/core_algos.py", line 1170, in compute_policy_loss
    pg_losses1 = -advantages * ratio
RuntimeError: The size of tensor a (3) must match the size of tensor b (6144) at non-singleton dimension 1
[36m(TaskRunner pid=3098789)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3102520, ip=172.18.49.219, actor_id=9bbffb7487d4de0a5a7ba68f01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14537a0cee60>)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/ray/base.py", line 466, in func
[36m(TaskRunner pid=3098789)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/base/decorator.py", line 501, in inner
[36m(TaskRunner pid=3098789)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/worker/fsdp_work.py", line 464, in update_actor
[36m(TaskRunner pid=3098789)[0m     metrics = self.actor.update_policy(data=data)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 78, in f
[36m(TaskRunner pid=3098789)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 88, in log
[36m(TaskRunner pid=3098789)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/workers/actor/dp_actor.py", line 392, in update_policy
[36m(TaskRunner pid=3098789)[0m     pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/trainer/ppo/core_algos.py", line 1170, in compute_policy_loss
[36m(TaskRunner pid=3098789)[0m     pg_losses1 = -advantages * ratio
[36m(TaskRunner pid=3098789)[0m RuntimeError: The size of tensor a (4) must match the size of tensor b (6144) at non-singleton dimension 1
[36m(TaskRunner pid=3098789)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3102519, ip=172.18.49.219, actor_id=4255fc43de9a97b461c198bb01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14aada38ee60>)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/ray/base.py", line 466, in func
[36m(TaskRunner pid=3098789)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/base/decorator.py", line 501, in inner
[36m(TaskRunner pid=3098789)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/worker/fsdp_work.py", line 464, in update_actor
[36m(TaskRunner pid=3098789)[0m     metrics = self.actor.update_policy(data=data)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 78, in f
[36m(TaskRunner pid=3098789)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 88, in log
[36m(TaskRunner pid=3098789)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/workers/actor/dp_actor.py", line 392, in update_policy
[36m(TaskRunner pid=3098789)[0m     pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/trainer/ppo/core_algos.py", line 1170, in compute_policy_loss
[36m(TaskRunner pid=3098789)[0m     pg_losses1 = -advantages * ratio
[36m(TaskRunner pid=3098789)[0m RuntimeError: The size of tensor a (3) must match the size of tensor b (6144) at non-singleton dimension 1
[36m(TaskRunner pid=3098789)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3102342, ip=172.18.49.219, actor_id=3e7c7b38f0b7b5b3baec279a01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x142b8ab6ae60>)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/ray/base.py", line 466, in func
[36m(TaskRunner pid=3098789)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/single_controller/base/decorator.py", line 501, in inner
[36m(TaskRunner pid=3098789)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/e3/worker/fsdp_work.py", line 464, in update_actor
[36m(TaskRunner pid=3098789)[0m     metrics = self.actor.update_policy(data=data)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 78, in f
[36m(TaskRunner pid=3098789)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/utils/debug/performance.py", line 88, in log
[36m(TaskRunner pid=3098789)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/workers/actor/dp_actor.py", line 392, in update_policy
[36m(TaskRunner pid=3098789)[0m     pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
[36m(TaskRunner pid=3098789)[0m   File "/scratch/gautschi/alochab/E3-RL4LLMs/verl/verl/trainer/ppo/core_algos.py", line 1170, in compute_policy_loss
[36m(TaskRunner pid=3098789)[0m     pg_losses1 = -advantages * ratio
[36m(TaskRunner pid=3098789)[0m RuntimeError: The size of tensor a (3) must match the size of tensor b (6144) at non-singleton dimension 1
