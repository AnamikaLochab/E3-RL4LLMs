# from transformers.modeling_flash_attention_utils import _flash_attention_forward
# print(_flash_attention_forward)
# import flash_attn 
# print(flash_attn.__version__)
import torch
print("torch:", torch.__version__, "CUDA:", torch.version.cuda)